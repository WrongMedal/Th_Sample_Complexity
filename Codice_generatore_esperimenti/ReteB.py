# -*- coding: utf-8 -*-
"""12. Esperimenti Rete B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pgArHW9A55amDps4zO8-L9cJZJI0WJ5-

#Rete B con grafici
"""

#@title Install

!pip install pytorch-lightning --quiet
!pip install wandb -qU

#@title Import

#General
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F

#Data
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torchvision.datasets import MNIST
from torchvision import transforms

#Valutazioni e grafica
from torchmetrics import Accuracy
import seaborn as sns
import matplotlib as plt

#Logging & Callbacks
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.callbacks import ModelCheckpoint
import wandb
from pytorch_lightning.loggers import WandbLogger
wandb.login()

#@title Feedforward Neural Network B
class FNN_B(pl.LightningModule):
#Rete con primo layer con pesi NON allenabili, temp indica le dimensioni di questo primo layer
    def __init__(self, temp, hidd_layers_dim, seme):
        super().__init__()

        #Accuracy come attributo
        self.train_acc = Accuracy(task="multiclass", num_classes=10)
        self.valid_acc = Accuracy(task="multiclass", num_classes=10)
        self.test_acc = Accuracy(task="multiclass", num_classes=10)

        input_size = 28*28 #Dim of MNIST data
        output_size = 10 #Number of classes
        #In questo modo la rete ha pesi NON ALLENABILI & FISSI PER SEED
        with torch.random.fork_rng(devices=[]):  # Forks RNG state
            torch.manual_seed(seme)
            self.backbone = nn.Linear(input_size, temp)

        self.backbone.requires_grad_(False)
        layers = [nn.Flatten(),self.backbone]

        for hidd_dim in hidd_layers_dim:
            layers.append(nn.Linear(temp, hidd_dim))
            layers.append(nn.ReLU())
            temp = hidd_dim

        layers.append(nn.Linear(temp, output_size))
        self.model = nn.Sequential(*layers)
        self._init_weights()

    #Inizializzazione metodo Xavier dei pesi del layer non addestrabile
    def _init_weights(self):
        nn.init.xavier_uniform_(self.backbone.weight)

    def forward(self, x):
        return self.model(x)

    #Seguendo paradigma lightning
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)   #softmax è implicito
        preds = torch.argmax(logits, dim=1) #la classe più prob.

        self.train_acc.update(preds, y)
        self.log("train_loss", loss, prog_bar=True)
        return loss #backprop, agg.gradiente e optimizer impliciti con lightning

    def on_training_epoch_end(self):
        self.log("train_acc", self.train_acc.compute(), prog_bar=True)
        self.train_acc.reset()

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        preds = torch.argmax(logits, dim=1)

        self.valid_acc.update(preds, y)
        self.log("valid_loss", loss, prog_bar=True)
        return loss

    def on_validation_epoch_end(self):
        self.log("valid_acc", self.valid_acc.compute(), prog_bar=True)
        self.valid_acc.reset()

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.cross_entropy(logits, y)
        preds = torch.argmax(logits, dim=1)
        self.test_acc.update(preds, y)
        self.log("test_loss", loss, prog_bar=True)
        return loss

    def on_test_epoch_end(self):
        self.log("test_acc", self.test_acc.compute(), prog_bar=True)
        self.test_acc.reset()

    def configure_optimizers(self):
        return torch.optim.SGD(self.parameters(), lr=0.001)

#@title MnistDataModule

class MnistDataModule(pl.LightningDataModule):
    def __init__(self, seme, num_samples, data_path='./'):
        super().__init__()
        self.data_path = data_path #Percorso in cui scarico MNIST
        self.num_samples = num_samples
        self.seme = seme
        self.transform = transforms.Compose([transforms.ToTensor()]) #Immagine convertita in tensore Pytorch


    def prepare_data(self):
        MNIST(root=self.data_path, download=True)

    def setup(self, stage=None):
        mnist_all = MNIST(
            root=self.data_path,
            train=True,
            transform=self.transform,
            download=False
        )

        val_size = 5000
        full_size = 60000
        train_size = self.num_samples
        rest = full_size - val_size - train_size

        generator = torch.Generator().manual_seed(self.seme)
        full_split = random_split(
            mnist_all,
            [train_size, rest, val_size],
            generator=generator
        )

        self.train = full_split[0]
        self.val = full_split[2]  # ultimo blocco è sempre la validazione

        self.test = MNIST(
            root=self.data_path,
            train=False,
            transform=self.transform,
            download=False
        )

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=64, num_workers=2)

    def val_dataloader(self):
        return DataLoader(self.val, batch_size=64, num_workers=2)

    def test_dataloader(self):
        return DataLoader(self.test, batch_size=64, num_workers=2)

#@title Funzione utilitaria per esperimenti
def run_experiment(seme, sample_size, layer, epochs, num_proj):
    pl.seed_everything(seme, workers=True)
    mnist_dm = MnistDataModule(seme, num_samples=sample_size)
    L = layer
    reteB = FNN_B(L, (layer, layer, layer), seme)
    wandb_logger = WandbLogger(
    project=f"Exp_{num_proj} Rete B",
    group=f"sample{sample_size}_layers{layer}",
    name=f"sample_{sample_size}",
    config={
        "sample_size": sample_size,
        "layer_structure": layer,
        "seed": seme
    }
)

    checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints/",
    save_top_k=2,
    monitor="valid_acc",
    mode="max"
)
    """
    early_stopping = EarlyStopping(
    monitor="valid_acc",
    min_delta=0.00,
    patience=5,
    verbose=False,
    mode="max"
)"""
    trainer = pl.Trainer(
    max_epochs = epochs,
    callbacks=[checkpoint_callback], #early_stopping,
    log_every_n_steps=5,
    logger=wandb_logger,
    deterministic=True
)
    trainer.fit(model=reteB, datamodule=mnist_dm)
    trainer.test(model=reteB, datamodule=mnist_dm)
    wandb.finish()

#@title Params Expn
"""
num_proj = #numero dell'esperimento
epochs = #epoche richieste
semi = (0, 1, 42)
layers = #tupla dei layer per rete
sample_size = #lista dei sample size (<= 550000)
"""

#@title Esperimenti
for seme in semi:
    for layer in layers:
        for size in sample_size:
            run_experiment(seme, size, layer, epochs, num_proj)
